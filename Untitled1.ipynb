{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f291253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009ce1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trajectory = collections.namedtuple('Trajectory', 'states actions rewards dones frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc102bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_v0(env_id):\n",
    "    env = gym.make(env_id)\n",
    "    dataset = env.get_dataset()\n",
    "    obs, acs, rs, dones =\\\n",
    "        dataset['observations'], dataset['actions'], dataset['rewards'], dataset['terminals']\n",
    "\n",
    "    def _parse(obs,actions,rewards,dones,trim_first_T,max_episode_steps):\n",
    "        trajs = []\n",
    "        start = trim_first_T\n",
    "        while start < len(dones):\n",
    "            end = start\n",
    "\n",
    "            while end != 1000000 - 1 and end < len(dones) - 1 and \\\n",
    "                (not dones[end] and end - start + 1 < max_episode_steps):\n",
    "                end += 1\n",
    "\n",
    "            if dones[end]:\n",
    "                # the trajectory ends normally.\n",
    "                # since the next state will not be (should not be, actually) used by any algorithms,\n",
    "                # we add null states (zero-states) at the end.\n",
    "\n",
    "                traj = Trajectory(\n",
    "                    states = np.concatenate([obs[start:end+1],np.zeros_like(obs[0])[None]],axis=0),\n",
    "                    actions = actions[start:end+1],\n",
    "                    rewards = rewards[start:end+1],\n",
    "                    dones = dones[start:end+1].astype(np.bool_),\n",
    "                    frames = None,\n",
    "                )\n",
    "\n",
    "                assert np.all(traj.dones[:-1] == False) and traj.dones[-1]\n",
    "\n",
    "            else:\n",
    "                # episodes end unintentionally (terminate due to timeout, cut-off when concateante two trajectories, or etc).\n",
    "                # since the next-state is not available, it drops the last action.\n",
    "\n",
    "                traj = Trajectory(\n",
    "                    states = obs[start:end+1],\n",
    "                    actions = actions[start:end],\n",
    "                    rewards = rewards[start:end],\n",
    "                    dones = dones[start:end].astype(np.bool_),\n",
    "                    frames = None,\n",
    "                )\n",
    "\n",
    "                assert np.all(traj.dones == False)\n",
    "\n",
    "            if len(traj.states) > 1: # some trajectories are extremely short in -medium-replay dataset (due to unexpected timeout caused by RLKIT); https://github.com/rail-berkeley/d4rl/issues/86#issuecomment-778566671\n",
    "                trajs.append(traj)\n",
    "\n",
    "            start = end + 1\n",
    "\n",
    "        return trajs\n",
    "\n",
    "    if env_id == 'halfcheetah-medium-replay-v0':\n",
    "        trajs = _parse(obs,acs,rs,dones,0,env._max_episode_steps)\n",
    "    elif env_id == 'halfcheetah-medium-v0':\n",
    "        trajs = _parse(obs,acs,rs,dones,899,env._max_episode_steps-1) # why env._max_episode_stpes - 1? it is questionable, but it looks a valid thing to do.\n",
    "    elif env_id == 'halfcheetah-expert-v0':\n",
    "        trajs = _parse(obs,acs,rs,dones,996,env._max_episode_steps-1)\n",
    "    elif env_id == 'halfcheetah-medium-expert-v0':\n",
    "        trajs = _parse(obs[:1000000],acs[:1000000],rs[:1000000],dones[:1000000],899,env._max_episode_steps-1) + \\\n",
    "            _parse(obs[1000000:],acs[1000000:],rs[1000000:],dones[1000000:],996,env._max_episode_steps-1)\n",
    "    elif env_id == 'hopper-medium-v0':\n",
    "        trajs = _parse(obs,acs,rs,dones,211,env._max_episode_steps)\n",
    "    elif env_id == 'hopper-expert-v0':\n",
    "        trajs = _parse(obs,acs,rs,dones,309,env._max_episode_steps-1)\n",
    "    elif env_id == 'hopper-medium-expert-v0': # actually, expert + mixed\n",
    "        trajs = _parse(obs[:1000000],acs[:1000000],rs[:1000000],dones[:1000000],309,env._max_episode_steps-1) + \\\n",
    "            _parse(obs[1000000:],acs[1000000:],rs[1000000:],dones[1000000:],0,env._max_episode_steps-1)\n",
    "    elif env_id == 'walker2d-medium-v0':\n",
    "        trajs = _parse(obs,acs,rs,dones,644,env._max_episode_steps)\n",
    "    elif env_id == 'walker2d-expert-v0':\n",
    "        trajs = _parse(obs,acs,rs,dones,487,env._max_episode_steps-1)\n",
    "    elif env_id == 'walker2d-medium-expert-v0': # actually, expert + mixed\n",
    "        trajs = _parse(obs[:1000000],acs[:1000000],rs[:1000000],dones[:1000000],644,env._max_episode_steps) + \\\n",
    "            _parse(obs[1000000:],acs[1000000:],rs[1000000:],dones[1000000:],487,env._max_episode_steps-1)\n",
    "    elif env_id in ['halfcheetah-random-v0', 'walker2d-random-v0', 'hopper-random-v0', 'walker2d-medium-replay-v0', 'hopper-medium-replay-v0']:\n",
    "        trajs = _parse(obs,acs,rs,dones,0,env._max_episode_steps-1)\n",
    "    elif env_id in ['pen-expert-v0', 'hammer-expert-v0', 'door-expert-v0', 'relocate-expert-v0']:\n",
    "        trajs = _parse(obs,acs,rs,dones,0,env._max_episode_steps)\n",
    "    elif env_id in ['door-human-v0','relocate-human-v0','hammer-human-v0']:\n",
    "        trajs = _parse(obs,acs,rs,dones,0,1000)\n",
    "        for traj in trajs:\n",
    "            traj.dones[:] = False # this is philosophical decision; since its original env does not terminate, so 'done' in the human data does not meaning anything. I regard this information is given only as a trajectory separator.\n",
    "    elif env_id in ['door-cloned-v0','relocate-cloned-v0','hammer-cloned-v0']:\n",
    "        trajs = _parse(obs[:500000],acs[:500000],rs[:500000],dones[:500000],0,1000) + \\\n",
    "            _parse(obs[500000:],acs[500000:],rs[500000:],dones[500000:],0,env._max_episode_steps)\n",
    "        for traj in trajs:\n",
    "            traj.dones[:] = False # this is philosophical decision; since its original env does not terminate, so 'done' in the human data does not meaning anything. I regard this information is given only as a trajectory separator.\n",
    "    elif env_id in ['pen-human-v0']:\n",
    "        trajs = _parse(obs,acs,rs,np.zeros_like(dones),0,200)\n",
    "        for traj in trajs:\n",
    "            traj.dones[:] = False\n",
    "    elif env_id in ['pen-cloned-v0']:\n",
    "        trajs = _parse(obs[:250000],acs[:250000],rs[:250000],dones[:250000],0,200) + \\\n",
    "            _parse(obs[250000:],acs[250000:],rs[250000:],dones[250000:],0,env._max_episode_steps)\n",
    "    else:\n",
    "        trajs = _parse(obs,acs,rs,dones,0,env._max_episode_steps)\n",
    "\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b0c954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_S_A_R_D_NS_NA_from_trajs(trajs):\n",
    "    \n",
    "    s, a, r, d, ns, na = [], [], [], [], [], []\n",
    "    \n",
    "    action_dim = len(trajs[0].actions[0])\n",
    "    \n",
    "    for traj in trajs:\n",
    "        \n",
    "        traj_len = len(traj.rewards)\n",
    "        \n",
    "        for t in range(traj_len):\n",
    "            \n",
    "            if t == traj_len - 1:  # final timestep\n",
    "                if traj.dones[t]:  # ok to append a dummy next action; done prevents bootstrapping\n",
    "                    na.append(np.zeros((action_dim, )))\n",
    "                else:              # the final timestep should be discarded\n",
    "                    break          # start next trajectory\n",
    "            else:\n",
    "                na.append(traj.actions[t+1])\n",
    "            \n",
    "            s.append(traj.states[t])\n",
    "            a.append(traj.actions[t])\n",
    "            r.append(traj.rewards[t])\n",
    "            d.append(traj.dones[t])\n",
    "            ns.append(traj.states[t+1])\n",
    "    \n",
    "    return s, a, r, d, ns, na"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
